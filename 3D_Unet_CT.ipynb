{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/outofray/3D_Unet_CT/blob/main/3D_Unet_CT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF5C7dO16Y5x"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose, Conv3D, MaxPooling3D, Conv3DTranspose\n",
        "from keras.layers import Input, merge, UpSampling2D,BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io as io\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import random as r\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3djfsH3JPDz",
        "outputId": "412ac320-0abb-4946-ce3f-298730c6a14b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Yjxz5_6kWo",
        "outputId": "82e22f73-550d-499c-d99b-5d2e759cf89b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting simpleitk\n",
            "  Downloading SimpleITK-2.1.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.4 MB 31 kB/s \n",
            "\u001b[?25hInstalling collected packages: simpleitk\n",
            "Successfully installed simpleitk-2.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install simpleitk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umbf1Tjxp6LW"
      },
      "outputs": [],
      "source": [
        "import SimpleITK as sitk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqS76KtA6Y6A"
      },
      "source": [
        "# Convert CT image to Array "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuxhbFB26Y6A"
      },
      "outputs": [],
      "source": [
        "def to_array(path, end, z_slice_number=4):\n",
        "    \n",
        "    # get locations\n",
        "    files = glob(path+end, recursive=True)\n",
        "    \n",
        "    img_list = []\n",
        "    \n",
        "    r.seed(42)\n",
        "    r.shuffle(files)\n",
        "    \n",
        "    for file in files:\n",
        "        img = sitk.ReadImage(file)\n",
        "        # img= sitk.Shrink(img, [1, 4, 4])  #optional: resize the image size if you want to save memory\n",
        "\n",
        "        # to numpy array\n",
        "        img = sitk.GetArrayFromImage(img)\n",
        "\n",
        "        # standardization\n",
        "        img = (img-img.mean())/img.std()\n",
        "        img.astype(\"float32\")\n",
        "        \n",
        "        for slice in range(0, img.shape[0], z_slice_number):  #pack slice into 3D array\n",
        "\n",
        "          if slice+z_slice_number+1 < img.shape[0]:\n",
        "            img_s = img[slice:slice+z_slice_number,:,:]\n",
        "            img_s = np.expand_dims(img_s, axis=0)\n",
        "            img_list.append(img_s)\n",
        "          else:\n",
        "            break\n",
        "            \n",
        "    return np.array(img_list, np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX-yK5lO6Y6F"
      },
      "outputs": [],
      "source": [
        "def seg_to_array(path, end, label, z_slice_number=4):\n",
        "    \n",
        "    # get locations\n",
        "    files = glob(path+end, recursive=True)\n",
        "    \n",
        "    img_list = []\n",
        "    \n",
        "    r.seed(42)\n",
        "    r.shuffle(files)\n",
        "    \n",
        "    for file in files:\n",
        "        img = sitk.ReadImage(file)\n",
        "        # img= sitk.Shrink(img, [1, 4, 4]) #optional: resize the image size if you want to save memory\n",
        "\n",
        "        # to numpy array\n",
        "        img = sitk.GetArrayFromImage(img)\n",
        "\n",
        "        # current segmentation as label 1, keep if statement for future target\n",
        "        if label == 1:\n",
        "            img[img == 1]\n",
        "            \n",
        "\n",
        "        img.astype(\"float32\")\n",
        "        \n",
        "        for slice in range(0, img.shape[0], z_slice_number):\n",
        "          if slice+z_slice_number+1<img.shape[0]:\n",
        "\n",
        "            img_s = img[slice:slice+z_slice_number,:,:]\n",
        "            \n",
        "            img_s = np.expand_dims(img_s, axis=0)\n",
        "            img_list.append(img_s)\n",
        "          else:\n",
        "            break\n",
        "            \n",
        "    return np.array(img_list,np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset(csv_file, folder, dataset):\n",
        "  df = pd.read_csv(csv_file)\n",
        "  df = df[df[\"DATASET\"]==dataset]\n",
        "  dataset_ID = df['ID'].squeeze().tolist()\n",
        "\n",
        "  arr = np.empty((0,1,16,512,512))\n",
        "  for ID in dataset_ID:\n",
        "    path = folder+ID\n",
        "    if \"LABEL\" in path:\n",
        "      data = seg_to_array(path=path, z_slice_number=16, end=\"_label.nii.gz\")\n",
        "    data = to_array(path=path, z_slice_number=16, end=\".nii.gz\")\n",
        "    arr = np.append(data, arr, axis=0)\n",
        "\n",
        "  return arr"
      ],
      "metadata": {
        "id": "POdGnH0I0wlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert training set data to array\n",
        "--same process to validation/test set"
      ],
      "metadata": {
        "id": "e5z_EkNNFbvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcfQ1SivC2OD"
      },
      "outputs": [],
      "source": [
        "# read image to array\n",
        "csv_file = \"/content/gdrive/MyDrive/your classification file list.csv\"\n",
        "folder = \"/content/gdrive/MyDrive/your image folder/\"\n",
        "dataset = 1 #train=1, val=2, test=3\n",
        "\n",
        "train = read_dataset(csv_file, folder, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read seg to array\n",
        "csv_file = \"/content/gdrive/MyDrive/your classification file list.csv\"\n",
        "folder = \"/content/gdrive/MyDrive/your segmentation label folder/\"\n",
        "\n",
        "dataset = 1 #train=1, val=2, test=3\n",
        "\n",
        "train_seg = read_dataset(csv_file, folder, dataset)"
      ],
      "metadata": {
        "id": "Bmuyml1TFIAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmYpksuI6Y6H"
      },
      "source": [
        "# 3D U-NET MODEL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coef(y_true, y_pred):\n",
        "    smooth = 0.005 \n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1-dice_coef(y_true, y_pred)"
      ],
      "metadata": {
        "id": "HQlknoICFuyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9Q_ScRC6Y6J",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "\n",
        "def unet():\n",
        "    \n",
        "    inputs = Input((1, 16, 32, 32))\n",
        "    \n",
        "    conv1 = Conv3D(64, (3, 3, 3), activation='relu', padding='same') (inputs)\n",
        "    batch1 = BatchNormalization(axis=1)(conv1)\n",
        "    conv1 = Conv3D(64, (3, 3, 3), activation='relu', padding='same') (batch1)\n",
        "    batch1 = BatchNormalization(axis=1)(conv1)\n",
        "    pool1 = MaxPooling3D((2, 2, 2), padding='same')(batch1)\n",
        "\n",
        "    conv2 = Conv3D(128, (3, 3, 3), activation='relu', padding='same') (pool1)\n",
        "    batch2 = BatchNormalization(axis=1)(conv2)\n",
        "    conv2 = Conv3D(128, (3, 3, 3), activation='relu', padding='same') (batch2)\n",
        "    batch2 = BatchNormalization(axis=1)(conv2)\n",
        "    pool2 = MaxPooling3D((2, 2, 2), padding='same')(batch2)\n",
        "\n",
        "    conv3 = Conv3D(256, (3, 3, 3), activation='relu', padding='same') (pool2)\n",
        "    batch3 = BatchNormalization(axis=1)(conv3)\n",
        "    conv3 = Conv3D(256, (3, 3, 3), activation='relu', padding='same') (batch3)\n",
        "    batch3 = BatchNormalization(axis=1)(conv3)\n",
        "    pool3 = MaxPooling3D((2, 2, 2), padding='same')(batch3)\n",
        "\n",
        "    conv4 = Conv3D(512, (3, 3, 3), activation='relu', padding='same') (pool3)\n",
        "    batch4 = BatchNormalization(axis=1)(conv4)\n",
        "    conv4 = Conv3D(512, (3, 3, 3), activation='relu', padding='same') (batch4)\n",
        "    batch4 = BatchNormalization(axis=1)(conv4)\n",
        "    pool4 = MaxPooling3D(pool_size=(2, 2, 2), padding='same')(batch4)\n",
        "\n",
        "    conv5 = Conv3D(1024, (3, 3, 3), activation='relu', padding='same') (pool4)\n",
        "    batch5 = BatchNormalization(axis=1)(conv5)\n",
        "    conv5 = Conv3D(1024, (3, 3, 3), activation='relu', padding='same') (batch5)\n",
        "    batch5 = BatchNormalization(axis=1)(conv5)\n",
        "\n",
        "    up6 = Conv3DTranspose(512, (2, 2, 2), strides=(2, 2, 2), padding='same') (batch5)\n",
        "    up6 = concatenate([up6, conv4], axis=1)\n",
        "    conv6 = Conv3D(512, (3, 3, 3), activation='relu', padding='same') (up6)\n",
        "    batch6 = BatchNormalization(axis=1)(conv6)\n",
        "    conv6 = Conv3D(512, (3, 3, 3), activation='relu', padding='same') (batch6)\n",
        "    batch6 = BatchNormalization(axis=1)(conv6)\n",
        "    \n",
        "    up7 = Conv3DTranspose(256, (2, 2, 2), strides=(2, 2, 2), padding='same') (batch6)\n",
        "    up7 = concatenate([up7, conv3], axis=1)\n",
        "    conv7 = Conv3D(256, (3, 3, 3), activation='relu', padding='same') (up7)\n",
        "    batch7 = BatchNormalization(axis=1)(conv7)\n",
        "    conv7 = Conv3D(256, (3, 3, 3), activation='relu', padding='same') (batch7)\n",
        "    batch7 = BatchNormalization(axis=1)(conv7)\n",
        "    \n",
        "    up8 = Conv3DTranspose(128, (2, 2, 2), strides=(2, 2, 2), padding='same') (batch7)\n",
        "    up8 = concatenate([up8, conv2], axis=1)\n",
        "    conv8 = Conv3D(128, (3, 3, 3), activation='relu', padding='same') (up8)\n",
        "    batch8 = BatchNormalization(axis=1)(conv8)\n",
        "    conv8 = Conv3D(128, (3, 3, 3), activation='relu', padding='same') (batch8)\n",
        "    batch8 = BatchNormalization(axis=1)(conv8)\n",
        "    \n",
        "    up9 = Conv3DTranspose(64, (2, 2, 2), strides=(2, 2, 2), padding='same') (batch8)\n",
        "    up9 = concatenate([up9, conv1], axis=1)\n",
        "    conv9 = Conv3D(64, (3, 3, 3), activation='relu', padding='same') (up9)\n",
        "    batch9 = BatchNormalization(axis=1)(conv9)\n",
        "    conv9 = Conv3D(64, (3, 3, 3), activation='relu', padding='same') (batch9)\n",
        "    batch9 = BatchNormalization(axis=1)(conv9)\n",
        "\n",
        "    conv10 = Conv3D(1, (1, 1, 1), activation='sigmoid')(batch9)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[conv10])\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=1e-4), loss=dice_coef_loss, metrics=[dice_coef])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8zaEq-AJ63q"
      },
      "outputs": [],
      "source": [
        "model = unet()\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59sxtPCf34JD"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsI0fYvsE5Xq"
      },
      "outputs": [],
      "source": [
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "\n",
        "loss_history = LossHistory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwqaI9RPVnFL"
      },
      "outputs": [],
      "source": [
        "checkpoint_filepath = \"/content/drive/MyDrive/best_model.hdf5\"\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath, \n",
        "    save_best_only=True, \n",
        "    monitor='val_loss', #based on best val_loss\n",
        "    mode='auto', verbose=1)\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,                                                  \n",
        "                                              patience=10, min_lr=1e-8)\n",
        "\n",
        "callbacks = [model_checkpoint, reduce_lr, loss_history]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPLqMQQF6Y6J",
        "outputId": "9f0de3eb-3e89-47cb-f8e0-82e508cbbfa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "   6/1332 [..............................] - ETA: 10:50 - loss: -0.0342 - dice_coef: 1.0342WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0223s vs `on_train_batch_end` time: 0.3895s). Check your callbacks.\n",
            "1332/1332 [==============================] - ETA: 0s - loss: -0.0798 - dice_coef: 1.0798"
          ]
        }
      ],
      "source": [
        "history = model.fit(new_train, new_train_seg, validation_split=0.25, batch_size=16, epochs=20, shuffle=True, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFt2QgzQJ1ln"
      },
      "outputs": [],
      "source": [
        "# plot train/validation dice_coef\n",
        "plt.plot(history.history['dice_coef'])\n",
        "plt.plot(history.history['val_dice_coef'])\n",
        "plt.title('Model Dice')\n",
        "plt.ylabel('Dice')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# plot train/validation loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Va.'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOBy6GyrldPu"
      },
      "source": [
        "\n",
        "# Visualizing Model prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/content/drive/MyDrive/best_model.hdf5\")"
      ],
      "metadata": {
        "id": "MN3EQlpy75iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting inference data\n",
        "i=200"
      ],
      "metadata": {
        "id": "MY8lO4S9mLD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(16):\n",
        "  fig=plt.figure(figsize=(9,9))\n",
        "\n",
        "  plt.subplot(1,3,1)   \n",
        "  plt.imshow(new_train[i][0][j]);\n",
        "\n",
        "  plt.subplot(1,3,2) \n",
        "  plt.imshow(new_train_seg[i][0][j]);\n",
        "\n",
        "  expand_img = np.expand_dims(train[i], axis=0)\n",
        "  pred = model.predict(expand_img)\n",
        "  super_threshold_indices = pred < 0.99\n",
        "  pred[super_threshold_indices] = 0\n",
        "\n",
        "  plt.subplot(1,3,3) \n",
        "  plt.imshow(pred[0][0][j]);\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "5Zy8Q5SkyRyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FybJt0CDlcpk"
      },
      "outputs": [],
      "source": [
        "for j in range(16):\n",
        "  fig=plt.figure(figsize=(9,9))\n",
        "\n",
        "  plt.subplot(1,3,1)   \n",
        "  plt.imshow(val[i][0][j]);\n",
        "\n",
        "  plt.subplot(1,3,2) \n",
        "  plt.imshow(val_seg[i][0][j]);\n",
        "\n",
        "  expand_img = np.expand_dims(val[i], axis=0)\n",
        "  pred = model.predict(expand_img)\n",
        "  super_threshold_indices = pred < 0.99\n",
        "  pred[super_threshold_indices] = 0\n",
        "\n",
        "  plt.subplot(1,3,3) \n",
        "  plt.imshow(pred[0][0][j]);\n",
        "\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "3D_Unet_CT",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}