{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7255ffc3",
   "metadata": {},
   "source": [
    "### 1. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87fe034d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os.path import join, isfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "from math import ceil, floor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import albumentations as A\n",
    "import tqdm\n",
    "from model import UNet3d, get_loss, get_dice\n",
    "\n",
    "# Configuration\n",
    "csv_ff = './data/list.csv'\n",
    "imgnii_path = './image/'\n",
    "segnii_path = './image/'\n",
    "epoch = 350\n",
    "loss_type = 'dicefocalLosssigmoid'\n",
    "cosine = 1\n",
    "lr = 5e-4\n",
    "pretrain = 0\n",
    "model_name = 'pneumoperitoneum'\n",
    "\n",
    "# Checking pretrain model existence\n",
    "pretrain_str = int(isfile(pretrain))\n",
    "\n",
    "# Experiment directory setup\n",
    "exp_dir = join('model', datetime.now().strftime(\"%m%d-%H%M%S\"))\n",
    "model_ff = join(exp_dir, 'unet3d.pt')\n",
    "\n",
    "# Create experiment directory if it doesn't exist\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "def normalize(data):\n",
    "    # Normalize data to range [0, 1]\n",
    "    data_min = np.min(data)\n",
    "    data_max = np.max(data)\n",
    "    return (data - data_min) / (data_max - data_min)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfea2424",
   "metadata": {},
   "source": [
    "### 2. Data checking and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44a673",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 34/40 [00:33<00:07,  1.30s/it]"
     ]
    }
   ],
   "source": [
    "# Load CSV file\n",
    "df = pd.read_csv(csv_ff)\n",
    "\n",
    "# Split dataframe by REV column for Train, Validation, and Test datasets\n",
    "Train_ana = df[df['REV'] == 1]\n",
    "Valid_ana = df[df['REV'] == 2]\n",
    "Test_ana = df[df['REV'] == 3]\n",
    "\n",
    "# Further split datasets by LABEL for positive and negative cases\n",
    "Train_ana_P = Train_ana[Train_ana['LABEL'] == 1]\n",
    "Valid_ana_P = Valid_ana[Valid_ana['LABEL'] == 1]\n",
    "Test_ana_P = Test_ana[Test_ana['LABEL'] == 1]\n",
    "\n",
    "Train_ana_N = Train_ana[Train_ana['LABEL'] == 0]\n",
    "Valid_ana_N = Valid_ana[Valid_ana['LABEL'] == 0]\n",
    "Test_ana_N = Test_ana[Test_ana['LABEL'] == 0]\n",
    "\n",
    "# Prepare data for summary DataFrame\n",
    "df_3D = [\n",
    "        [Train_ana_P.shape[0], Train_ana_N.shape[0], Train_ana.shape[0]],\n",
    "        [Valid_ana_P.shape[0], Valid_ana_N.shape[0], Valid_ana.shape[0]],\n",
    "        [Test_ana_P.shape[0], Test_ana_N.shape[0], Test_ana.shape[0]]\n",
    "        ]\n",
    "\n",
    "# Creating a summary DataFrame\n",
    "df_3D_sum2 = pd.DataFrame(df_3D, columns=['(+)', '(-)', 'Total'],\n",
    "                          index=['Train', 'Valid', 'Test'])\n",
    "\n",
    "# Lists for storing dataset information\n",
    "train_list = []\n",
    "val_list = []\n",
    "test_list = []\n",
    "train_label = []\n",
    "val_label = []\n",
    "test_label = []\n",
    "\n",
    "# Iterate through DataFrame to process images and masks\n",
    "for ii in tqdm.tqdm(range(df.shape[0])):\n",
    "    Train_valid_test = df.iloc[ii]['REV']\n",
    "    accid = df.iloc[ii]['ACCID']\n",
    "    label = df.iloc[ii]['LABEL']\n",
    "    \n",
    "    # Construct paths to image and mask\n",
    "    image = join(imgnii_path, f'{accid}.nii.gz')\n",
    "    mask = join(segnii_path, f'{accid}_label.nii.gz')\n",
    "    \n",
    "    # Load and normalize image and mask volumes\n",
    "    image_vol = normalize(nib.load(image).get_fdata()).astype(np.float16)  # Normalization\n",
    "    mask_vol = nib.load(mask).get_fdata().astype(np.int8)\n",
    "    \n",
    "    # Append data to corresponding lists based on dataset type\n",
    "    if Train_valid_test == 1:\n",
    "        train_list.append([image_vol, mask_vol, label])\n",
    "        train_label.append(label)\n",
    "    elif Train_valid_test == 2:\n",
    "        val_list.append([image_vol, mask_vol, label])\n",
    "        val_label.append(label)\n",
    "    elif Train_valid_test == 3:\n",
    "        test_list.append([image_vol, mask_vol, label])\n",
    "        test_label.append(label)\n",
    "\n",
    "# Print counts of labels for each dataset\n",
    "print('Train label 0/1:', np.bincount(train_label))\n",
    "print('Val label 0/1:', np.bincount(val_label))\n",
    "print('Test label 0/1:', np.bincount(test_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196185c",
   "metadata": {},
   "source": [
    "### 3. Create Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7184f37c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Getdata():\n",
    "    # Class for data management and augmentation\n",
    "    def __init__(self, data_list, training=True):\n",
    "        self.data = data_list\n",
    "        self.training = training\n",
    "        # Define augmentation pipeline\n",
    "        self.aug = A.Compose([\n",
    "            A.LongestMaxSize(max_size=512), \n",
    "            A.CropNonEmptyMaskIfExists(384, 384),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return the length of data\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get item by index\n",
    "        data = {}       \n",
    "        vol, mask, label = self.data[idx]\n",
    "\n",
    "        if self.training:\n",
    "            # Apply augmentations if training\n",
    "            temp = self.aug(image=vol.astype(np.float32), mask=mask.astype(np.int16))\n",
    "            vol = temp['image']\n",
    "            mask = temp['mask']\n",
    "        \n",
    "        # Prepare data dictionary\n",
    "        data['vol'] = vol[None, ...]  # Add channel dimension\n",
    "        data['mask'] = mask\n",
    "        data['label'] = label\n",
    "        return data\n",
    "\n",
    "# Instantiate data handlers\n",
    "train_data = Getdata(train_list, training=True)\n",
    "val_data = Getdata(val_list, training=False)\n",
    "test_data = Getdata(test_list, training=False)\n",
    "\n",
    "# Example access to the data\n",
    "data = train_data.__getitem__(1)  # Accessing data for demonstration\n",
    "print('The content of data:', data['vol'].shape, data['mask'].shape)\n",
    "\n",
    "# DataLoader instances for batching\n",
    "train_data_t = DataLoader(train_data, shuffle=True, num_workers=0)\n",
    "val_data_t = DataLoader(val_data, shuffle=False, num_workers=0)\n",
    "test_data_t = DataLoader(test_data, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a31364-4c70-46ce-bbe3-d8a5968cc33b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Setting up GPU and building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4dfd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clear any cached memory, useful when GPUs are running out of memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Determine the device to run the model on (GPU or CPU)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load a pretrained model if available; otherwise, initialize a new model\n",
    "if isfile(pretrain):   \n",
    "    print(f'Loading pretrained model: {pretrain}')\n",
    "    NET = torch.load(pretrain).to(device)\n",
    "else:                  \n",
    "    print('No pretrained model found, initializing a new model.')\n",
    "    NET = UNet3d(in_channels=1, n_classes=2).to(device)  # Default channel = 16\n",
    "\n",
    "# Set up the optimizer with model parameters and learning rate\n",
    "optimizer = torch.optim.Adam(NET.parameters(), lr=lr)  \n",
    "optimizer.zero_grad()  # Reset gradients to zero for a fresh start\n",
    "\n",
    "# Define a learning rate scheduler for optimizing training\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8, eta_min=3e-6)\n",
    "\n",
    "# Print training dataset size and total number of epochs\n",
    "total_n = len(train_data)\n",
    "print(f'Total training data sets: {total_n}')\n",
    "print(f'Total epochs: {epoch}')\n",
    "\n",
    "# Determine the computing device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Display additional information if using CUDA (GPU)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory_gb = round(torch.cuda.memory_allocated(0) / (1024**3), 1)  # Convert bytes to GB\n",
    "    cached_memory_gb = round(torch.cuda.memory_reserved(0) / (1024**3), 1)  # Convert bytes to GB\n",
    "    print(f'Allocated: {allocated_memory_gb} GB')\n",
    "    print(f'Cached:    {cached_memory_gb} GB')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59555f05",
   "metadata": {},
   "source": [
    "### 5. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adca4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_validateloss(test_data_d):\n",
    "    # Validate model performance on test data\n",
    "    with torch.no_grad():\n",
    "        loss_list = []\n",
    "        dice_list = []\n",
    "        pbar = tqdm.tqdm(test_data_d)\n",
    "        for data in pbar:\n",
    "            vol_d = data['vol'].to('cuda').float()\n",
    "            logits = NET(vol_d)\n",
    "            mask = data['mask']\n",
    "            loss = get_loss(logits, mask.to(device))\n",
    "            loss_list.append(loss.item())\n",
    "            # Predict mask based on the specified loss type\n",
    "            mask_pred = torch.sigmoid(logits)[0, 1, ...].cpu().detach().numpy() > 0.5\n",
    "            mask_true = mask[0, ...].numpy()\n",
    "            dice = get_dice(mask_pred, mask_true)\n",
    "            dice_list.append(dice)\n",
    "            pbar.set_description(f\"Loss: {loss:.3f}, Dice: {dice:.3f}, pixel: {np.sum(mask_pred)}\")\n",
    "\n",
    "    return np.mean(loss_list), np.mean(dice_list)\n",
    "\n",
    "# Initialize metrics and logs\n",
    "max_valid_dice = 0.0  # Initialize max valid dice, consider saving/testing for dice > 0.5\n",
    "min_valid_loss = 0.9  # Initialize min valid loss threshold\n",
    "\n",
    "# Initialize logs for training and validation metrics\n",
    "train_loss_log = np.empty(0)  # Log for training loss\n",
    "val_loss_log = np.empty(0)    # Log for validation loss\n",
    "test_loss_log = np.empty(0)   # Log for test loss\n",
    "ep_log = np.empty(0)          # Log for epochs (general)\n",
    "ep_log2 = []                  # Log for epochs (validation specific)\n",
    "ep_log3 = []                  # Log for epochs (test specific)\n",
    "\n",
    "# Training loop\n",
    "for ep in range(1, epoch + 1):\n",
    "    print('Epoch: ', ep, f'Training...{model_ff}')\n",
    "    print('Learning rate: %.7f' % optimizer.param_groups[0]['lr'])\n",
    "    loss_list = []  # List to store loss for each batch\n",
    "\n",
    "    pbar = tqdm.tqdm(train_data_t)\n",
    "    NET.train()  # Set model to training mode\n",
    "    for data in pbar:\n",
    "        vol_d = data['vol'].to(device).float()        \n",
    "        logits = NET(vol_d)\n",
    "\n",
    "        label1 = data['label']\n",
    "        label2 = torch.squeeze(label1, 0)\n",
    "        mask = data['mask']\n",
    "\n",
    "        # Conditionally modify the mask based on label\n",
    "        if label2.numpy() == 0:\n",
    "            mask2 = mask.numpy()\n",
    "            mask3 = np.zeros(shape=mask2.shape).astype(int)\n",
    "            mask = torch.tensor(mask3)\n",
    "\n",
    "        # Calculate and log the loss\n",
    "        loss = get_loss(logits, mask.to(device))\n",
    "        loss_list.append(loss.item())\n",
    "        pbar.set_description(\"Loss %.3f\" % loss.item())\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update model parameters\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "    # Log training loss for epoch\n",
    "    train_loss = np.mean(loss_list)\n",
    "    train_loss_log = np.append(train_loss_log, train_loss)\n",
    "    print(f'Train Loss {ep: 5d}/{epoch}: {train_loss: 2.5f}')\n",
    "    \n",
    "    # Perform validation at specified epochs\n",
    "    if ep % 1 == 0:  # Adjust condition based on preference for validation frequency\n",
    "        NET.eval()  # Set model to evaluation mode\n",
    "        print('Validation.....')\n",
    "        val_loss, val_dice = get_validateloss(val_data_t)\n",
    "        print(f'Valid Loss {ep: 5d}/{epoch}: {val_loss: 2.5f}')\n",
    "        print(f'Valid Dice {ep: 5d}/{epoch}: {val_dice: 2.5f}')   \n",
    "\n",
    "        # Update validation logs\n",
    "        val_loss_log = np.append(val_loss_log, val_loss)\n",
    "        ep_log2.append(ep)\n",
    "        # Save model and perform testing if validation loss improves\n",
    "        if val_loss < min_valid_loss:\n",
    "            min_valid_loss = val_loss\n",
    "            model_file = model_ff.replace('.pt', f'_ep{ep}_valdice{val_dice:.3f}.pt')\n",
    "            print(f'Writing model file {model_file}')\n",
    "            torch.save(NET, model_file)\n",
    "\n",
    "            # Perform JIT tracing for deployment\n",
    "            trace = torch.jit.trace(NET, vol_d)\n",
    "            torch.jit.save(trace, model_file.replace('.pt', '.pth'))\n",
    "            best_pth = model_file.replace('.pt', '.pth')\n",
    "\n",
    "            # Perform testing and log test metrics\n",
    "            print('Testing.....')\n",
    "            test_loss, test_dice = get_validateloss(test_data_t)\n",
    "            print(f'Test Dice {ep: 5d}/{epoch}: {test_dice: 2.5f}')\n",
    "            test_loss_log = np.append(test_loss_log, test_loss)\n",
    "            ep_log3.append(ep)\n",
    "\n",
    "    # Adjust learning rate if using a scheduler\n",
    "    if cosine:\n",
    "        scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
